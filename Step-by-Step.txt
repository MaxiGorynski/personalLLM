Build an LLM from Scratch

Introduction & Chapter 1

- LLMs are developments within NLP
- Use many millions or billions of parameters in order to obtain and maintain contextual command, and generate original text outputs of value. This level of contextual command defeated the scale and model architecture formerly usable for simpler NLP tasks; for instance, you could classify articles from the Iris flower dataset using a model of only two parameters, rather than the billions commanded by LLMs like Llama etc.

- Building this one using Python and Pytorch as our chosen tensor

- LLMs trained on vast quantities of text data - tens of billions of parameters, which are the adjustable weights in the network, adjusted during training, to predict the next word output
- Language is inherently sequential in nature, so surprisingly effective models can be produced with the simple principle of next-word prediction

Transformer
- Transformers work by paying selective attention to different parts of an input when making predictions, making them adept at handling language nuance
- A given architectural hierarchy:
    - 1. LLM, a deep neural network for parsing and generating text
    - 2. Deep learning, machine learning with a many-layered neural network
    - 3. Machine learning, algorithms that learn rules from data
    - 4. AI, systems with human-like intelligence
- To illustrate the principle of ML, a spam filter will, instead of relying on written rules to identify spam emails, will be fed examples of spam and legitimate emails, and by minimising error in predictions from a training dataset, will learn the characteristic differences.
    - It involves manual feature extraction - a human operator will manually extract certain features within the text, like trigger words or grammatical aberrations, or suspicious links, to train the model
- To illustrate the principle of deep learning by contrast, manual feature extraction is not required.

- Most contemporary LLMs are coded using the PyTorch deep learning library
- Custom LLMs can frequently outstrip general ones in performance
- General process involves pretraining and fine-tuning
- Pretraining: Model is trained on a large, diverse dataset to establish understanding of language
    - Pretrained models (foundation models) trained on unlabelled text are good at text completion and few-shot capabilities (i.e. it can lear to perform new tasks based on only a few examples, instead of needing extensive training data) and even zero-shot capabilities (i.e. learning to perform new tasks having never processed any specific examples)
- Finetuning: Model is trained specifically on narrower, more domain-specific datasets to finesse its performance
    - Finetuned models trained on labelled datasets can be excellent at classifications summarisation, translation etc.
    - Fine-tuning has two categories: instruction fine-tuning, and classification fine-tuning.
        - The former, the labelled dataset consists of instruction and answer pairs, like a query to translate a text followed by the correctly translated text
        - The latter, the labelled dataset consists of texts and associated class labels - for instance, emails associated with “spam” and “not spam” labels
- In pretraining, LLMs use self-supervised learning, where the model generates its own labels from input data

Transformer Architecture

- Deep neural network architecture introduced in “Attention is All You Need” (2017)
- Two submodules:
    - Encoder: Processes input text and encodes it into a series of numerical representations/vectors (embeddings) to capture contextual information
    - Decoder: Takes these and generates output text
    - In translation task, for instance, the encoder would decode the text from source lang into vectors, decoder would decode them to create text in the target lang
- Both modules consist of many layers connected by a self-attention mechanism (SAM). SAM allows the model to weigh the importance of different tokens relative to each other. Allows the model to capture long-range dependencies and contextual relationships within the input data
- EXAMPLE - BERT. BERT, trained via masked word prediction - where the model predicts words hidden in a sentence - has distinct strengths in text classification tasks e.g. sentiment prediction, document categorisation. Twitter uses it to detect toxic content
- BERT focuses on encoder (assessing input for missing components). GPT focuses on decoder (generating new components on the end of an original input or prompt).

- Transformers are not all LLMs, and LLMs and are not all transformers. Transformers can also be used for computer vision. And some LLMs ca be based on recurrent and convolutional architectures, instead of transformers.

Large Datasets for Training
- The likes of BERT and GPT are training on text corpora of billions of words, including natural and computing languages.
- Pre-training for GPT-3 involved the following (though of the 499 billion tokens enumerated, only 300 billion were used for actual training:
    - CommonCrawl - Web crawl data - 410 billion tokens - 60% of training data
    - WebText2 - Web crawl data - 19 billion tokens - 22% of training data
    - Books1 and Books2 - internet-based book corpus - 67 billion tokens - 16% of training data
    - Wikipedia - High-quality text - 3 billion tokens - 3% of training data
- Publicly available dataset is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024. May contain copyrighted works, exact usage terms will depend on the intended use case and country
- Pretraining of this kind will make for extraordinarily versatile LLMs. Pretraining requires access to significant resources and is very expensive. $4.6 million of cloud computing credits were required to train GPT-3.
- Luckily, LLMs can serve as foundation models and be fine-tuned on specific tasks with much smaller datasets, reducing computational resources needed and improving performance
- After implementing pretraining code here, we will learn how to reuse openly available model weights ad load them into the architecture we will implement

GPT Architecture
- GPT, originally introduced in “Improving Language Understanding by Generative Pre-Training” by Radford et al. from OpenAI; GPT-3 is a scaled-up version. ChatGPT created by fine-tuning GPT-3 on large instruction dataset using a method from OpenAI’s InstructGPT paper.
- Remarkable powers of spelling correction, translation, classification given they’re simply retrained on simple next-word prediction tasks
- Next word prediction is self-supervised learning, a form of self-labelling. Means we don’t need to collect labels for the training data. Can use the structure of the data itself; the next word in a sentence or document can be used as the label that the model is supposed to predict.
- Ext word prediction thus allows us to use massive unlabelled text datasets to train LLMs
- GPT architecture is simple - decoder part of a transformer with no encoder.
- Models like this that predict one word at a time are known as autoregressive, incorporating previous outputs as inputs for future predictions. Each new word is chosen based on the sequence that precedes it
- GPT-3 is significantly larger than original transformer - original trans. Repeated encode/decode six times. GPT-3 has 96 transformer layers and 175 billion params
- More recent models, like Meta’s Llama models, are still based on these concepts
- Show evidence of emergent behaviour - can perform tasks it was’t explicitly trained to perform, recognising patterns not in the dataset, because of the volume of vast qualities of linguistic data in diverse contexts

Three Stages of Coding a Language Model
- Stage 1 (Implementing sampling and basic mechanisms): Data prep and sampling; attention mechanism; LLM architecture
- Interstage 1: Pretraining
- Stage 2 (Pretrains LLM on unlabelled data to obtain a foundation model for future fine-tuning): Training loop; model evaluation; loading of retrained weights
- Interstage 2: Fine-tuning the pretrained LLM to create classification or chat model
- Stage 3: Classifier (for classification pre-training, dataset with class labels) or Personal Assistant (for chat pre-training, instruction dataset)


Chapter 2 - Working with Text Data

- Before we can train and finetune LLMs, we need to prepare the training dataset
- This involves splitting text data into individual word and subword tokens, before encoding them into vector embeddings
- Will also cover advanced tokenisation schema like byte pair encoding, used by LLMs like GPT

- Using specific neural network layers or models, we can embed different kinds of data — text, video, audio
- Embedding is mapping from discrete objects to points in a continuous vector space
- Embeddings can function at the word, sentence, paragraph, or document level. Sentence/para embeddings are used for retireval-augmented generation, which combines generation (like producing text) with retrieval (like searching and external knowledge base).
- We will focus on word embeddings

- Early embedding algo was Word2Vec. Allows for generation of word embeddings by predicting the context of a word given the target word. Words appearing in similar contexts are clustered closer in a vector space.
- Word embeddings can have varying dimensions, from one to thousands. Higher dimensionality captured nuanced relationships but reduces computational efficiency
- LLMs commonly produce their own embeddings that are part of the input layer, and are updated during training. This is better than using Word2Vec, because the embeddings are optimised to the specific task and data at hand

- High-dimensional embeddings are problematic because we can only conceive in three dimensions. Scatterplots are common for graphical representations of embeddings in a vector space. But even small GPT models can use embedding sizes of 768 dimensions.

Tokenising Edith Wharton’s “The Verdict”
- In simple tokenisation, encoding whitespaces as separate characters or just removing them depends on memory availability and computing power

Converting Tokens into Token IDs:
- Before converting token IDs to vectors, we must convert tokens from Python stringers to integer representations
- The Complete training dataset acts ask input text -> Tokenisation breaks input text down into individual tokens -> Tokenised training dataset; the vocab contains all unique tokens in the training set, sorted alphabetically -> Each token is added to the vocab I alphabetical order, mapped to a unique integer, token ID
- When converting LLM outputs back into text, we must turn token IDs into text, which can be done through a inverse version of the same vocabulary expression
- Having instantiated a new tokeniser and tested one’s encoder and decoder to success, if one tries to test it next on a word not in the vocab, a KeyError will result. Highlights the need for large training sets.

Adding Special Context Tokens
- Need to modify tokeniser to handle unknown words
- Also need to address usage/addition of special context tokens to enhance a model’s understanding of context. For instance, <|unk|> represents new and unknown words not part of training set.
- <|endoftext|> can separate two unrelated text sources. When they are then concatenated, the LLM has a stronger notion of how to process and understand the contexts they contain.
- Other special tokens include:
    - [BOS] (beginning of sequence) - Token marks the start of a text.
    - [EOS] (end of sequence) - Token marks end of a text, useful when concatenating multiple unrelated text
    - [PAD] (padding) - When training LLMs with batch-sizes larger than 1, the match might contain texts of differing lengths. To ensure all texts have the same length, shorter ones are padded using the [PAD] token to elongate them.

Byte Pair Encoding
- More sophisticated tokenisation scheme. Complex, best to use open source lib called tiktoken, implementing a NPE algo based on source code in Rust.
- Presuming installation of the dependency, we can run code like this

tokeniser = tiktoken.get_encoding("gpt2")

text = ("Hello, do you like tea? <|endoftext|> In the sunlit terraces""of someunknownPlace.")
integers = tokeniser.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
strings = tokeniser.decode(integers)
print(strings)

- Based on encoded ids and decoded text, we can see that with BPE <|endoftext|> is assigned a large token ID (the last number in BPE’s token vocabulary), and that BPE can encode and decode unknown words successfully
- How does BPE do this? It breaks down words that aren’t in its predefined vocab into sequences of smaller subwords/characters, enabling the handling of out-of-vocab words. For instances:

text = ("Akwirw <|endoftext|> ier.")
integers = tokeniser.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
strings = tokeniser.decode(integers)
print(strings)

- …produces the following output:

		   “Ak”   “w”   “ir” “w”  “ ”   “<|e…”   “ “    “ier” “.”
Integers: [33901, 86, 343, 86, 220, 50256, 220, 959, 13]
Strings: Akwirw <|endoftext|> ier.

- BPE builds its vocabulary by iteratively merging frequent characters into sub-frequent words ad frequent subwords into words.
- Starts with all individual single characters in vocabulary. Then, merges together character combos that frequently occur into subwords.

Data sampling with sliding window algorithm
- Next step in creating LLM embeddings is generating input-target pairs required for training
- For instance, take the sentence “LLMs learn to predict one word at a time.”  We tokenise and then create blocks as subsamples, “LLMs learn”, “LLMs learn to”, “LLMs learn to predict” etc. Based on training against lots of subsamples, next token prediction can be made effective. Subsamples are inputs; the next word is the target.
- Easy way to create input-target pairs is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by one.
- After you’ve created your input pairs for LLM training, we need to implement an efficient data loader that iterates over the input dataset and returns inputs/targets as PyTorch tensors (multidimensional arrays).
- We want two tensors - an input tensor containing the text the LLM sees, and a target tensor that includes targets for the LLM to predict. To implement, we collect inputs in a tensor ‘x’, which each row represents one input context; a second tensor, y, contains the corresponding prediction targets, created by shifting input one word to the right.
- We can first create a GPTDatasetV1 based on PyTorch Datrasert class, defining how rows are fetched from the dataset and load in equal-length batches determined by batch_size and max_length parameters. Our ‘stride’ parameter determines how many spots to the right our Python iterator will move (with next() function) to select the next target id.
    - For instance, a stride of 1 means that tensors of the sentence (“In the heart of the city stood the old library…”) would look like this:
        - tensor1: “In the heart of”
        - tensor2: “the heart of the”
    - …whereas a stride of 4 would lead to tensors looking like so:
        - tensor2: “In the heart of”
        - Tensor2: “the city stood the”
- Our first_batch variable will return two tensors - the first stores the input token IDs; the second stores the target token IDs:

[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]

- This has an input size of just 4. Most LLM training is done with input sizes of at least 256. It was also done with a batch_size of 1 and a batch size of 1. This is how the tensors look given an input size of 8, a batch_size of 2, and a stride of 2:

[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],
        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],
        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]

- Small batch sizes require less memory in training but lead to noises model updates.
- Setting stride length to match your max_length can be useful to avoid overlapping, which can lead to overfitting

Creating token embeddings
- Final step in preparing the input text for LLM training - converting token IDs to embedding vectors
- First, initialise embedding weights with random values. This is the starting point for the LLM learning process
- Continuous vector representation (i.e. embedding) is needed since LLMs are deep neural networks trained with backpropagation algo.
- If we set up a simple token-id-to-embedding conversion script, we can see something interesting

input_ids = torch.tensor([2,3,5,1])
vocab_size = 6
output_dim = 3

torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)

Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)

- The weight matrix of the embedding layer shown through the print statement shows a bunch of small random values. These will be adjusted an optimised through training.
- 6 rows, 3 columns; a row per possible tokens in the vocab, a column for each of the three embedding dimensions
- Applying a token to the embedding vector:

print(embedding_layer(torch.tensor([3])))

- …returns tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>) … as expected, this matched the fourth row (“3” in the table, starting from 0 as we do in Python).
- In other words, embedding layer is a lookup operation, retrieving rows from the weight matrix using a token ID
- Having created our embedding vectors from token IDs, next we’ll encode positional information about a token within a text

Encoding word positions
- Shortcoming of LLMs - their self-attention mechanism doesn’t have a notion of sequential position or order for the tokens in a sequence, only vector mapping. Good for reproducibility
- We ca fix this with two categories of position-aware embeddings: relative positional embeddings (RPE) and absolute positional embeddings (APE)
- APE are directly associated with sequence position; for each position in an input sequence, an embedding is added to the token’s embedding to convey its location. For instance, let’s say we have input embeddings of 2.1, 2.2, 2.3 in token embedding 1. Positional embeddings 1.1, 1.2, 1.3 might be attached to the input embeddings to communicate their position within the token
- RPE focuses on position or distance between tokens. The model learns the relationships in terms of how far apart a given token is from another given token, rather than precise position
- Choosing between them is application- and data-dependent.
- Having focused on very small embedding sizes, let’s scale up and look at realistically sized embeddings - input tokens in 256-dimensional vector representation, smaller than GPT-3 (12,288 dimensions) but still fine. Using BPE’s 50,257 vocab size
- Sampling data from the data loader, we will end up with a batch size of 8, four tokens each, and 256-dims: an 8 x 4 x 256 tensor

The innput embedding pipeline
- Input text (“This is an example”), goes to
- Tokenised text (“This”, “is” etc.), goes to
- Token IDs (40134, 2052 etc.), goes to
- Token embeddings, goes to
- Positional embeddings, goes to
- Input embeddings, fed into
- GPT-like decoder-only transformer
- Postprocessing and output tex


Chapter 3 - Coding attention mechanisms

- Our input text is prepared for training. Now, time for attention mechanisms.
- Four different variants:
    - 1. Simplified self-attention
    - 2. Self-attention (w. Trainable weights that forms the basis of mechanism in LLMs)
    - 3. Causal attention (allows a model to consider only previous and current inputs in a sequence, guaranteeing temporal order)
    - 4. Multi-head attention (Allows model to simultaneously attend to info from different subspaces)

The problem with long sequences
- Without an attention mechanism, LLMs are not possible. For example, in a translation task, you can’t translate from one language word by word; grammar/syntax differs etc.
- Before transformers, recurrent neural networks were the popular encoder/decoder architecture. In RNN, outputs from prior steps are as inputs to the current step, processed sequentially. The encoder updates its hidden state at each step, trying to capture sentence meaning in the final hidden state. The decoder then takes the final hidden state to generate the translated sentence one word at a time.
- However, these RNNs can only access one state — the most recent one — at a time. Earlier hidden states are inaccessible. Context is easily lost therefore the longer the token chain.

Capturing data dependencies with attention mechanisms
- The Bahdanau attention mechanism was first developed in 2014. It allows the RNN decoder to selectively access different parts of the input sequence at each decoding step.
- By accessing any input token selectively, and based on attention weights, the attention mechanism can arrogate higher output importance to some tokens than others.

Attending to different parts of the input with self-attention
- ‘Self’ in self-attention refers to mechanism’s ability to compute attention weights by relating different positions within a single input sequence. Assesses and learns relationships/dependencies between input substates (i.e. words) in one sequence. This is as opposed to other attention mechanisms, which focus on the difference between multiple sequences as whole units.
- Components of self-attention without trainable weights:
    - Input vector (token embedding) 1, first token
    - Attention weight to weigh the importance of IV1
    - Goal: Creating context vector for IV1, computed as a combo of all input vectors weighed with respect to IV1
    - Input vector (token embedding) 2, second token
    - Etc.
    - etc.
- Input text: “Your journey stars with one step.” Sequence consists of T elements, from x^1 to x^T, and each corresponds to a d-dimensional embedding vector representing a specific token, such as “Your”, “journey” etc.
- We want to calculate context vectors for each element x. Context creates an enriched representation of an element based on other elements; the relevance of words in a sentence to each other
- Weights help LLMs construct these context vectors, but that is for later
- How do we do this? We create an input sequence, embedded in three-dimensional vectors.

import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

- First, we calculate intermediate values, or attention scores, by calculating the dot product of the query (the second token in the sequence) with every other input token. Dot product is a concise way of multiplying two vectors element-wise and then summing the products, like so 
		res = 0.
		for idx, element in enumerate(inputs[0]):
			res += inputs[0][idx] * query [idx]
		print(res)
		print(torch.dot(inputs[0], query))

	…dot product also measures similarity by quantifying how closely two vectors are aligned. Higher dot product
	means higher similarity.

- We get the attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
- We then normalise the attention scores to obtain attention weights that sum up to 1. Useful for maintaining training stability in an LLM. Softmax is the best approach for normalisation, when managing extreme values. Ensures attention weights are always positive.
- Softmax can, however, suffer overflow/underflow when dealing with large/small inputs. Use PyTorch’s optimised softmax.

attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())

- Finally, we calculate the context vector by multiplying the embedded input tokens with corresponding attention weights, then summing the resulting vectors.
- Each input token gets its own turn as the embedded query token, which is used to calculate z^T, the context vector for that EQT.
- We calculate the context vectors for all of our input tokens, instead of just one.
    - First we compute attention scores (dot product between input vectors)
    - Then we compute attention weights, which are normalised versions of attention scores
    - Then we compute context vectors, a weighted sum over the inputs
- We do this like so:

attn_scores = torch.empty(6, 6) #For computing vectors for all six words in the sentence
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)

tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

- Each element in the tensor represents an attention score between each pair of inputs (one acting as key [rows] and one acting as query [columnns]). These are normalised.
- We can use ‘for’ loops (slower), or matrix multiplication

attn_scores = inputs @ inputs.T
#print(attn_scores)


- Then we normalise each row so that the values in each row sum to 1. We set dims to -1 as a means of instructing the soft max function to apply normalisation along the last dimension of the attn_scores tensor.

attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)

tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])

-

