Build an LLM from Scratch

Introduction & Chapter 1

- LLMs are developments within NLP
- Use many millions or billions of parameters in order to obtain and maintain contextual command, and generate original text outputs of value. This level of contextual command defeated the scale and model architecture formerly usable for simpler NLP tasks; for instance, you could classify articles from the Iris flower dataset using a model of only two parameters, rather than the billions commanded by LLMs like Llama etc.

- Building this one using Python and Pytorch as our chosen tensor

- LLMs trained on vast quantities of text data - tens of billions of parameters, which are the adjustable weights in the network, adjusted during training, to predict the next word output
- Language is inherently sequential in nature, so surprisingly effective models can be produced with the simple principle of next-word prediction

Transformer
- Transformers work by paying selective attention to different parts of an input when making predictions, making them adept at handling language nuance
- A given architectural hierarchy:
    - 1. LLM, a deep neural network for parsing and generating text
    - 2. Deep learning, machine learning with a many-layered neural network
    - 3. Machine learning, algorithms that learn rules from data
    - 4. AI, systems with human-like intelligence
- To illustrate the principle of ML, a spam filter will, instead of relying on written rules to identify spam emails, will be fed examples of spam and legitimate emails, and by minimising error in predictions from a training dataset, will learn the characteristic differences.
    - It involves manual feature extraction - a human operator will manually extract certain features within the text, like trigger words or grammatical aberrations, or suspicious links, to train the model
- To illustrate the principle of deep learning by contrast, manual feature extraction is not required.

- Most contemporary LLMs are coded using the PyTorch deep learning library
- Custom LLMs can frequently outstrip general ones in performance
- General process involves pretraining and fine-tuning
- Pretraining: Model is trained on a large, diverse dataset to establish understanding of language
    - Pretrained models (foundation models) trained on unlabelled text are good at text completion and few-shot capabilities (i.e. it can lear to perform new tasks based on only a few examples, instead of needing extensive training data) and even zero-shot capabilities (i.e. learning to perform new tasks having never processed any specific examples)
- Finetuning: Model is trained specifically on narrower, more domain-specific datasets to finesse its performance
    - Finetuned models trained on labelled datasets can be excellent at classifications summarisation, translation etc.
    - Fine-tuning has two categories: instruction fine-tuning, and classification fine-tuning.
        - The former, the labelled dataset consists of instruction and answer pairs, like a query to translate a text followed by the correctly translated text
        - The latter, the labelled dataset consists of texts and associated class labels - for instance, emails associated with “spam” and “not spam” labels
- In pretraining, LLMs use self-supervised learning, where the model generates its own labels from input data

Transformer Architecture

- Deep neural network architecture introduced in “Attention is All You Need” (2017)
- Two submodules:
    - Encoder: Processes input text and encodes it into a series of numerical representations/vectors (embeddings) to capture contextual information
    - Decoder: Takes these and generates output text
    - In translation task, for instance, the encoder would decode the text from source lang into vectors, decoder would decode them to create text in the target lang
- Both modules consist of many layers connected by a self-attention mechanism (SAM). SAM allows the model to weigh the importance of different tokens relative to each other. Allows the model to capture long-range dependencies and contextual relationships within the input data
- EXAMPLE - BERT. BERT, trained via masked word prediction - where the model predicts words hidden in a sentence - has distinct strengths in text classification tasks e.g. sentiment prediction, document categorisation. Twitter uses it to detect toxic content
- BERT focuses on encoder (assessing input for missing components). GPT focuses on decoder (generating new components on the end of an original input or prompt).

- Transformers are not all LLMs, and LLMs and are not all transformers. Transformers can also be used for computer vision. And some LLMs ca be based on recurrent and convolutional architectures, instead of transformers.

Large Datasets for Training
- The likes of BERT and GPT are training on text corpora of billions of words, including natural and computing languages.
- Pre-training for GPT-3 involved the following (though of the 499 billion tokens enumerated, only 300 billion were used for actual training:
    - CommonCrawl - Web crawl data - 410 billion tokens - 60% of training data
    - WebText2 - Web crawl data - 19 billion tokens - 22% of training data
    - Books1 and Books2 - internet-based book corpus - 67 billion tokens - 16% of training data
    - Wikipedia - High-quality text - 3 billion tokens - 3% of training data
- Publicly available dataset is Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining Research by Soldaini et al. 2024. May contain copyrighted works, exact usage terms will depend on the intended use case and country
- Pretraining of this kind will make for extraordinarily versatile LLMs. Pretraining requires access to significant resources and is very expensive. $4.6 million of cloud computing credits were required to train GPT-3.
- Luckily, LLMs can serve as foundation models and be fine-tuned on specific tasks with much smaller datasets, reducing computational resources needed and improving performance
- After implementing pretraining code here, we will learn how to reuse openly available model weights ad load them into the architecture we will implement

GPT Architecture
- GPT, originally introduced in “Improving Language Understanding by Generative Pre-Training” by Radford et al. from OpenAI; GPT-3 is a scaled-up version. ChatGPT created by fine-tuning GPT-3 on large instruction dataset using a method from OpenAI’s InstructGPT paper.
- Remarkable powers of spelling correction, translation, classification given they’re simply retrained on simple next-word prediction tasks
- Next word prediction is self-supervised learning, a form of self-labelling. Means we don’t need to collect labels for the training data. Can use the structure of the data itself; the next word in a sentence or document can be used as the label that the model is supposed to predict.
- Ext word prediction thus allows us to use massive unlabelled text datasets to train LLMs
- GPT architecture is simple - decoder part of a transformer with no encoder.
- Models like this that predict one word at a time are known as autoregressive, incorporating previous outputs as inputs for future predictions. Each new word is chosen based on the sequence that precedes it
- GPT-3 is significantly larger than original transformer - original trans. Repeated encode/decode six times. GPT-3 has 96 transformer layers and 175 billion params
- More recent models, like Meta’s Llama models, are still based on these concepts
- Show evidence of emergent behaviour - can perform tasks it was’t explicitly trained to perform, recognising patterns not in the dataset, because of the volume of vast qualities of linguistic data in diverse contexts

Three Stages of Coding a Language Model
- Stage 1 (Implementing sampling and basic mechanisms): Data prep and sampling; attention mechanism; LLM architecture
- Interstage 1: Pretraining
- Stage 2 (Pretrains LLM on unlabelled data to obtain a foundation model for future fine-tuning): Training loop; model evaluation; loading of retrained weights
- Interstage 2: Fine-tuning the pretrained LLM to create classification or chat model
- Stage 3: Classifier (for classification pre-training, dataset with class labels) or Personal Assistant (for chat pre-training, instruction dataset)


Chapter 2 - Working with Text Data

- Before we can train and finetune LLMs, we need to prepare the training dataset
- This involves splitting text data into individual word and subword tokens, before encoding them into vector embeddings
- Will also cover advanced tokenisation schema like byte pair encoding, used by LLMs like GPT

- Using specific neural network layers or models, we can embed different kinds of data — text, video, audio
- Embedding is mapping from discrete objects to points in a continuous vector space
- Embeddings can function at the word, sentence, paragraph, or document level. Sentence/para embeddings are used for retireval-augmented generation, which combines generation (like producing text) with retrieval (like searching and external knowledge base).
- We will focus on word embeddings

- Early embedding algo was Word2Vec. Allows for generation of word embeddings by predicting the context of a word given the target word. Words appearing in similar contexts are clustered closer in a vector space.
- Word embeddings can have varying dimensions, from one to thousands. Higher dimensionality captured nuanced relationships but reduces computational efficiency
- LLMs commonly produce their own embeddings that are part of the input layer, and are updated during training. This is better than using Word2Vec, because the embeddings are optimised to the specific task and data at hand

- High-dimensional embeddings are problematic because we can only conceive in three dimensions. Scatterplots are common for graphical representations of embeddings in a vector space. But even small GPT models can use embedding sizes of 768 dimensions.

Tokenising Edith Wharton’s “The Verdict”
- In simple tokenisation, encoding whitespaces as separate characters or just removing them depends on memory availability and computing power

Converting Tokens into Token IDs:
- Before converting token IDs to vectors, we must convert tokens from Python stringers to integer representations
- The Complete training dataset acts ask input text -> Tokenisation breaks input text down into individual tokens -> Tokenised training dataset; the vocab contains all unique tokens in the training set, sorted alphabetically -> Each token is added to the vocab I alphabetical order, mapped to a unique integer, token ID
- When converting LLM outputs back into text, we must turn token IDs into text, which can be done through a inverse version of the same vocabulary expression
- Having instantiated a new tokeniser and tested one’s encoder and decoder to success, if one tries to test it next on a word not in the vocab, a KeyError will result. Highlights the need for large training sets.

Adding Special Context Tokens
- Need to modify tokeniser to handle unknown words
- Also need to address usage/addition of special context tokens to enhance a model’s understanding of context. For instance, <|unk|> represents new and unknown words not part of training set.
- <|endoftext|> can separate two unrelated text sources. When they are then concatenated, the LLM has a stronger notion of how to process and understand the contexts they contain.
- Other special tokens include:
    - [BOS] (beginning of sequence) - Token marks the start of a text.
    - [EOS] (end of sequence) - Token marks end of a text, useful when concatenating multiple unrelated text
    - [PAD] (padding) - When training LLMs with batch-sizes larger than 1, the match might contain texts of differing lengths. To ensure all texts have the same length, shorter ones are padded using the [PAD] token to elongate them.

Byte Pair Encoding
- More sophisticated tokenisation scheme. Complex, best to use open source lib called tiktoken, implementing a NPE algo based on source code in Rust.
- Presuming installation of the dependency, we can run code like this

tokeniser = tiktoken.get_encoding("gpt2")

text = ("Hello, do you like tea? <|endoftext|> In the sunlit terraces""of someunknownPlace.")
integers = tokeniser.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
strings = tokeniser.decode(integers)
print(strings)

- Based on encoded ids and decoded text, we can see that with BPE <|endoftext|> is assigned a large token ID (the last number in BPE’s token vocabulary), and that BPE can encode and decode unknown words successfully
- How does BPE do this? It breaks down words that aren’t in its predefined vocab into sequences of smaller subwords/characters, enabling the handling of out-of-vocab words. For instances:

text = ("Akwirw <|endoftext|> ier.")
integers = tokeniser.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
strings = tokeniser.decode(integers)
print(strings)

- …produces the following output:

		   “Ak”   “w”   “ir” “w”  “ ”   “<|e…”   “ “    “ier” “.”
Integers: [33901, 86, 343, 86, 220, 50256, 220, 959, 13]
Strings: Akwirw <|endoftext|> ier.

- BPE builds its vocabulary by iteratively merging frequent characters into sub-frequent words ad frequent subwords into words.
- Starts with all individual single characters in vocabulary. Then, merges together character combos that frequently occur into subwords.

Data sampling with sliding window algorithm
- Next step in creating LLM embeddings is generating input-target pairs required for training
- For instance, take the sentence “LLMs learn to predict one word at a time.”  We tokenise and then create blocks as subsamples, “LLMs learn”, “LLMs learn to”, “LLMs learn to predict” etc. Based on training against lots of subsamples, next token prediction can be made effective. Subsamples are inputs; the next word is the target.
- Easy way to create input-target pairs is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by one.
- After you’ve created your input pairs for LLM training, we need to implement an efficient data loader that iterates over the input dataset and returns inputs/targets as PyTorch tensors (multidimensional arrays).
- We want two tensors - an input tensor containing the text the LLM sees, and a target tensor that includes targets for the LLM to predict. To implement, we collect inputs in a tensor ‘x’, which each row represents one input context; a second tensor, y, contains the corresponding prediction targets, created by shifting input one word to the right.
- We can first create a GPTDatasetV1 based on PyTorch Datrasert class, defining how rows are fetched from the dataset and load in equal-length batches determined by batch_size and max_length parameters. Our ‘stride’ parameter determines how many spots to the right our Python iterator will move (with next() function) to select the next target id.
    - For instance, a stride of 1 means that tensors of the sentence (“In the heart of the city stood the old library…”) would look like this:
        - tensor1: “In the heart of”
        - tensor2: “the heart of the”
    - …whereas a stride of 4 would lead to tensors looking like so:
        - tensor2: “In the heart of”
        - Tensor2: “the city stood the”
- Our first_batch variable will return two tensors - the first stores the input token IDs; the second stores the target token IDs:

[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]

- This has an input size of just 4. Most LLM training is done with input sizes of at least 256. It was also done with a batch_size of 1 and a batch size of 1. This is how the tensors look given an input size of 8, a batch_size of 2, and a stride of 2:

[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],
        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],
        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]

- Small batch sizes require less memory in training but lead to noises model updates.
- Setting stride length to match your max_length can be useful to avoid overlapping, which can lead to overfitting

Creating token embeddings
- Final step in preparing the input text for LLM training - converting token IDs to embedding vectors
- First, initialise embedding weights with random values. This is the starting point for the LLM learning process
- Continuous vector representation (i.e. embedding) is needed since LLMs are deep neural networks trained with backpropagation algo.
- If we set up a simple token-id-to-embedding conversion script, we can see something interesting

input_ids = torch.tensor([2,3,5,1])
vocab_size = 6
output_dim = 3

torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)

Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)

- The weight matrix of the embedding layer shown through the print statement shows a bunch of small random values. These will be adjusted an optimised through training.
- 6 rows, 3 columns; a row per possible tokens in the vocab, a column for each of the three embedding dimensions
- Applying a token to the embedding vector:

print(embedding_layer(torch.tensor([3])))

- …returns tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>) … as expected, this matched the fourth row (“3” in the table, starting from 0 as we do in Python).
- In other words, embedding layer is a lookup operation, retrieving rows from the weight matrix using a token ID
- Having created our embedding vectors from token IDs, next we’ll encode positional information about a token within a text

Encoding word positions
- Shortcoming of LLMs - their self-attention mechanism doesn’t have a notion of sequential position or order for the tokens in a sequence, only vector mapping. Good for reproducibility
- We ca fix this with two categories of position-aware embeddings: relative positional embeddings (RPE) and absolute positional embeddings (APE)
- APE are directly associated with sequence position; for each position in an input sequence, an embedding is added to the token’s embedding to convey its location. For instance, let’s say we have input embeddings of 2.1, 2.2, 2.3 in token embedding 1. Positional embeddings 1.1, 1.2, 1.3 might be attached to the input embeddings to communicate their position within the token
- RPE focuses on position or distance between tokens. The model learns the relationships in terms of how far apart a given token is from another given token, rather than precise position
- Choosing between them is application- and data-dependent.
- Having focused on very small embedding sizes, let’s scale up and look at realistically sized embeddings - input tokens in 256-dimensional vector representation, smaller than GPT-3 (12,288 dimensions) but still fine. Using BPE’s 50,257 vocab size
- Sampling data from the data loader, we will end up with a batch size of 8, four tokens each, and 256-dims: an 8 x 4 x 256 tensor

The innput embedding pipeline
- Input text (“This is an example”), goes to
- Tokenised text (“This”, “is” etc.), goes to
- Token IDs (40134, 2052 etc.), goes to
- Token embeddings, goes to
- Positional embeddings, goes to
- Input embeddings, fed into
- GPT-like decoder-only transformer
- Postprocessing and output tex


Chapter 3 - Coding attention mechanisms

- Our input text is prepared for training. Now, time for attention mechanisms.
- Four different variants:
    - 1. Simplified self-attention
    - 2. Self-attention (w. Trainable weights that forms the basis of mechanism in LLMs)
    - 3. Causal attention (allows a model to consider only previous and current inputs in a sequence, guaranteeing temporal order)
    - 4. Multi-head attention (Allows model to simultaneously attend to info from different subspaces)

The problem with long sequences
- Without an attention mechanism, LLMs are not possible. For example, in a translation task, you can’t translate from one language word by word; grammar/syntax differs etc.
- Before transformers, recurrent neural networks were the popular encoder/decoder architecture. In RNN, outputs from prior steps are as inputs to the current step, processed sequentially. The encoder updates its hidden state at each step, trying to capture sentence meaning in the final hidden state. The decoder then takes the final hidden state to generate the translated sentence one word at a time.
- However, these RNNs can only access one state — the most recent one — at a time. Earlier hidden states are inaccessible. Context is easily lost therefore the longer the token chain.

Capturing data dependencies with attention mechanisms
- The Bahdanau attention mechanism was first developed in 2014. It allows the RNN decoder to selectively access different parts of the input sequence at each decoding step.
- By accessing any input token selectively, and based on attention weights, the attention mechanism can arrogate higher output importance to some tokens than others.

Attending to different parts of the input with self-attention
- ‘Self’ in self-attention refers to mechanism’s ability to compute attention weights by relating different positions within a single input sequence. Assesses and learns relationships/dependencies between input substates (i.e. words) in one sequence. This is as opposed to other attention mechanisms, which focus on the difference between multiple sequences as whole units.
- Components of self-attention without trainable weights:
    - Input vector (token embedding) 1, first token
    - Attention weight to weigh the importance of IV1
    - Goal: Creating context vector for IV1, computed as a combo of all input vectors weighed with respect to IV1
    - Input vector (token embedding) 2, second token
    - Etc.
    - etc.
- Input text: “Your journey stars with one step.” Sequence consists of T elements, from x^1 to x^T, and each corresponds to a d-dimensional embedding vector representing a specific token, such as “Your”, “journey” etc.
- We want to calculate context vectors for each element x. Context creates an enriched representation of an element based on other elements; the relevance of words in a sentence to each other
- Weights help LLMs construct these context vectors, but that is for later
- How do we do this? We create an input sequence, embedded in three-dimensional vectors.

import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

- First, we calculate intermediate values, or attention scores, by calculating the dot product of the query (the second token in the sequence) with every other input token. Dot product is a concise way of multiplying two vectors element-wise and then summing the products, like so 
		res = 0.
		for idx, element in enumerate(inputs[0]):
			res += inputs[0][idx] * query [idx]
		print(res)
		print(torch.dot(inputs[0], query))

	…dot product also measures similarity by quantifying how closely two vectors are aligned. Higher dot product
	means higher similarity.

- We get the attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
- We then normalise the attention scores to obtain attention weights that sum up to 1. Useful for maintaining training stability in an LLM. Softmax is the best approach for normalisation, when managing extreme values. Ensures attention weights are always positive.
- Softmax can, however, suffer overflow/underflow when dealing with large/small inputs. Use PyTorch’s optimised softmax.

attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())

- Finally, we calculate the context vector by multiplying the embedded input tokens with corresponding attention weights, then summing the resulting vectors.
- Each input token gets its own turn as the embedded query token, which is used to calculate z^T, the context vector for that EQT.
- We calculate the context vectors for all of our input tokens, instead of just one.
    - First we compute attention scores (dot product between input vectors)
    - Then we compute attention weights, which are normalised versions of attention scores
    - Then we compute context vectors, a weighted sum over the inputs
- We do this like so:

attn_scores = torch.empty(6, 6) #For computing vectors for all six words in the sentence
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)

tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

- Each element in the tensor represents an attention score between each pair of inputs (one acting as key [rows] and one acting as query [columnns]). These are normalised.
- We can use ‘for’ loops (slower), or matrix multiplication

attn_scores = inputs @ inputs.T
#print(attn_scores)


- Then we normalise each row so that the values in each row sum to 1. We set dims to -1 as a means of instructing the soft max function to apply normalisation along the last dimension of the attn_scores tensor.

attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)

tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])

- In the final step, we use the attention weights to compute all context vectors via matrix multiplication

Implementing Self-Attention with Trainable Weights
- Now, we will implement a self-attention mechanism. Popular in transformer architecture, GPT models etc.
- Called scaled dot-product attention. This is done to avoid small gradients; these are troublesome during back propagation, and can cause learning to slow down.
- Previously we coded a simplified attention mewchanism to understand the basic mechanism behind attention mechanisms. Now, we add trainable weights.
- We want to compute context vectors as weighted sums over the input vectors specific to a certain input element.
- Only slight differences to the self-attention mechanism already coded, just with the introduction of weight matrices

- Implementing self-attention step-by-step by introducing three trainable weight matrices, Wq, Wk, Wv. We compute them for each input element (x1, x2 etc).
- We designate query inputs/query tokens for each round, as before. This query vector is obtained via matrix multiplication between input (e.g. x2) and the weight matrix (Wq). We obtain key and value vectors doing the same thing using Wk and Wv.

#Initialise the three weight matrices, Wq for query, Wk for key, Wv for v value vectors
#Grad is set to false to reduce output clutter, but they should be set to True during model
#training so that matrices are updated
torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

query_2 = x_2 @ W_query
key_2 = x_2 @ W_key
value_2 = x_2 @ W_value
print(query_2)

Output:
tensor([0.4306, 1.4551])

- The output is a two-dimensional tensor because we set the number of columns in the weight matrix to 2 with d_out (even though the number of input was d=3; in most other attention scenarios both in and out will be the same!)
- Having obtained the query vector, we obtain all the keys and value vectors through matrix multiplication

keys = inputs @ W_key
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)

- The output will show that we’ve projected the six input tokens from a three-dimensional space into a two-dimensional embedding space
- Next step is computing attention scores
- As with the self-attention mechanism without weights, we calculate the attention score by performing dot-product using the query and key obtained by transforming the inoputs via their weight matrices
- First, compute the attention score (in this case for x2):

keys_2 = keys[1] #Performing this on x^2 to start, so skipping to index 1, where x^2 is held
#Dot product of the query vector by the key vector
attn_scores_22 = query_2.dot(keys_2)
print(attn_scores_22)

- Then, we generalise this using matrix multiplication
- After that, we go from attention scores to attention weights. We do this by scaling the attention scores (by dividing them by the square root of the embedding dimension of the keys) and using the softmax function

d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)
print(attn_weights_2)

- Finally, with all this in hand, we compute the context vectors; as before, a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. The weights determine the importance of each value vector.

#Calculating the context vectors using the attention weights with matrix multiplication
context_vec_2 = attn_weights_2 @ values
#print(context_vec_2)


- Now, let’s turn this weighted self-attention mechanism into a compact class

class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))

    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T  # omega
        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec

- This is derived from nn.Module, a vital PyTorch building block for model layer creation and management
- The __init__ initialises trainable weight matrices for queries, keys and values, before normalisation
- We can use the class like this:

torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))

- Can further improve our class by using PyTorch’s nn.Linear layers, which effectively handle matrix multiplication

class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores/ keys.shape[-1] ** 0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec

Hiding future words with causal attention
- Causal attention is a critical mechanism - stops the model accessing future words (as the prediction should depend only on prior words and current inputs). Also known as masked attention.
- We take attention scores and attention weights; we mask with ‘O’s above diagonal to produce Masked Attention Scores, then normalise them
- Compute attention weights for masked attention

queries = sa_v2.W_query(inputs)
keys = sa_v2.W_key(inputs)
attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1)
print(attn_weights)

- Use tril to create a mask where values above diagonal are zero

context_length = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(context_length, context_length))
print(mask_simple)

Output:
tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])

- Multiply the mask with the attention weights to zero-out results above the diagonal

masked_simple = attn_weights*mask_simple
print(masked_simple)

- Renormalise the attention weights to sum up to one

row_sums = masked_simple.sum(dim=-1, keepdim=True)
masked_simple_norm = masked_simple/row_sums
print(masked_simple_norm)

- Now, we’re ensuring that only unmasked (and therefore appropriate) input tokens are being assessed at a given time, in the order they appear in the input sequence. This is in spite of the fact that softmax uses all of the positions, including future ones, in the denominator.
- Let’s make this more efficiently — mask the attention scores with negative infinity values before applying softmax
- Now, we consider the concept of dropout. Randomly selected hidden layer units are ignored during training. Prevents overfitting.
- Dropout is applied after calculating attention weights, or after applying weights to the value vectors.
- When training a GPT model, we will use a lower dropout rate of 0.1 or 0.2.
- An example below, using just a set of '1's

#Apply dropout for attention weights to prevent overfitting
torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5)
example = torch.ones(6, 6)
print(dropout(example))

Output with dropout value of 0.5:

tensor([[2., 2., 2., 2., 2., 2.],
        [0., 2., 0., 0., 0., 0.],
        [0., 0., 2., 0., 2., 0.],
        [2., 2., 0., 0., 0., 2.],
        [2., 0., 0., 0., 0., 2.],
        [0., 2., 0., 0., 0., 0.]])

Output with dropout value of 0.2:

tensor([[1.2500, 1.2500, 1.2500, 1.2500, 1.2500, 1.2500],
        [1.2500, 1.2500, 1.2500, 0.0000, 1.2500, 1.2500],
        [0.0000, 1.2500, 1.2500, 1.2500, 1.2500, 1.2500],
        [1.2500, 1.2500, 0.0000, 1.2500, 1.2500, 1.2500],
        [1.2500, 0.0000, 0.0000, 1.2500, 0.0000, 1.2500],
        [1.2500, 1.2500, 0.0000, 0.0000, 1.2500, 0.0000]])

- You can see that when dropout is applied, the values of the remaining elements in the matrix are scaled up in order to maintain normalisation, and overall attention weight balance maintained, during both training and inference phases.
- Next, we apply dropout to the attention weight matrix itself

- With this done, we can incorporate causal attention and dropout modifications into our SelfAttention Python class. This will be our template for multi-head attention
- Need first to ensure that the code can handle batches consisting of more than one input, so that CausalAttention (class) can support DataLoader outputs. Then we create our new class:

class CausalAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
        super().__init__()
        #d_in = #Input feature dimensionality for each token (e.g., 512 for GPT models)
        self.d_out = d_out #Output dimensionality of the transformed token representations.
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) #Current token focus
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) #What current token is being matched againnst
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) #What values the tokens communicate
        self.dropout = nn.Dropout(dropout) #Dropout layer added from SelfAttention_v1 #Randomly zeroinng several tokens
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)
        ) #Values above diag are blocked

    def forward(self, x):
        b, num_tokens, d_in = x.shape #Set up input tensor's batch size, token number, dimensionality. We transpose dimensions 1 and 2, keeping the batch dimennsion at the first position (0)

        # Each linear layer transforms the input tensor x into keys, queries, and values
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.transpose(1, 2) #We transpose dimensions 1 and 2, keeping the batch dimennsion at the first position (0). Computes dot products between queries and keys for every pair of tokens in the sequence
        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #Prevent attention to future tokens by setting the corresponding attention scores to -inf.. Ops with a trailing underscore are performed in-place, avoiding unnecessary memory copies
        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1) #Normalisation via converting scores into probabilities.
        attn_weights = self.dropout(attn_weights)

        context_vec = attn_weights @ values #Each token's context vector is computed as a weighted sum of the values, where the weights are given by the attention probabilities.
        return context_vec

- We use register_buffer here so that tensors do not have to be on the same device as your model parameters.
- We have now implemented causal attention in our burgeoning neural network. Now it’s time to implement a multi-head attention model that can implement several causal attention mechanisms in parallel.

Multi-Head Attention

- Now we’re going to extend our causal attention class over multiple heads.
- First we’ll do this by stacking multiple Causal Attention models. Then we will implement it in a more complicated but computationally efficient way.
- In a stacked self-attention mechanism, the embedded input tokens remain unchanged. Instead of one value weight matrix and query matrix, however, we have two. We get two context vectors that we combine into a single one later.

class MultiHeadAttentionWrapper(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.heads = nn.ModuleList(
            [CausalAttention(
                d_in, d_out, context_length, dropout, qkv_bias
            )
                for _ in range(num_heads)]
        )

    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)

- If we use this class with two attention heads and an output dimension of two with CausalAttention, we will get a four-dimensional context vector

torch.manual_seed(123)
context_length = batch.shape[1] #Number of tokens
d_in, d_out = 3, 2
mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)

Output:
tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]],

        [[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)
context_vecs.shape: torch.Size([2, 6, 4])

- First dimension of the tensor above is 2, as we have two input texts (duplicated, hence why the context vectors are identical). Second dimension refers to the 6 tokens in each input. Third dimension refers to the four-dimensional embedding of each token.
- We understand how to use the forward function to execute each of these single-head attention modules in sequence. However, we can compute the output for all attention heads simultaneously using matrix multiplication.

Implementing multi-head attention with weight splits
- We merge MultiHeadAttention and CausalAttention classes into one, with mods for efficiency
- In MultiHeadAttention, multiple heads are implemented via a list of CausalAttention objects, representing a separate head.
- Splits input into multiple heads through reshaping the query, key and value tensors ad then combines them.

class MultiHeadAttention(nn.Module):
    # Class constructor
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()

        # Assert to ensure that output dimension is divisible by number of heads
        assert (d_out % num_heads == 0), "d_out must be divisible by num_heads"

        # Store output dimension, attenntion head number, dimensionality of each
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        # Initialise linear transformation layers of queries/keys/values
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Output projection after attention applied
        self.out_proj = nn.Linear(d_in, d_out)

        # Dropout layer for avoiding overfit
        self.dropout = nn.Dropout(dropout)

        # Mask to apply to scores for causal (self-attention) masking
        self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1)
                             )

        # Application of multi-head attention
        def forward(self, x):
            # Batch-size, number of tokens per sequences, embeddings size from input
            b, num_tokens, d_in = x.shape

            # Apply transformations to produce keys, queries, and values from x
            keys = self.W_key(x)
            queries = self.W_query(x)
            values = self.W_value(x)

            # Reshape each of these for the tensor shape determined above
            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
            values = values.view(b, num_tokens, self.num_heads, self.head_dim)
            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

            # Transpose to bring num_heads dimension to second axis for easier attention computation
            keys = keys.transpose(1, 2)
            queries = queries.transpose(1, 2)
            values = values.transpose(1, 2)

            # Compute attention scores through matrix mult of queries/keys
            attn_scores = queries @ keys.traspose(2, 3)

            # Create a boolean mask for the matrix above the diagonal
            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

            # Fill masked positions with negative infinity, prevents future tokens being seen
            attn_scores.masked_fill_(mask_bool, -torch.inf)

            # Normalise attention scores, apply softmax to get weights
            attn_weights = torch.softmax(
                attn_scores / keys.shape[-1] ** 0.5, dim=-1)

            # Apply dropout for regularisationn
            attn_weights = self.dropout(attn_weights)

            # Compute weighted sum of values using attention weights/matrix mult.
            context_vec = (attn_weights @ values).transpose(1, 2)

            # Rehsape context vector to original shape (batch_size, num_tokens, d_out)
            context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)

            # Final output projection to obtain result of multi-head attention
            context_vec = self.out_prok(context_vec)
            return context_vec

- The transformations of tensor shapes looks very mathematically complex, but the module above uses the same concept as the MultiHeadAttention wrapper from earlier.
- In the wrapper, we combined many single-headed attention layers into a single layer. Here, we start with a multi-head layer and then splits it into individual heads
- The query, key and value tensors are split and transformed for each separate head.
- Key operation is splitting embeddings output dimensions (d_out) into num_heads and head_dim
- To put it another way, in the Wrapper, we have inputs x, then two weight matrices (d_out = 2 in each), producing two query matrices. In the class, we have inputs x, fed into a combined weight matrix (d_out = 4). We matrix multiply for a combined query set, which are then split into different query matrices.
- We have to transpose our tensors carefully so that they eventually have the same shape (batch size, number of heads, number of tokens, and head output dimensions) so that they are properly aligned for batching.
- Tensor shape parameters like batch size, number of attention heads, number of tokens per sequence, and embeddings size are metadata for the tensor; all tensor values are embeddings themselves corresponding to these params.

- We have used much smaller embedding sizes and attention head numbers to keep the outputs readable here.
- The smallest GPT model (117 million params) has 12 heads and a context vector embedding size of 768.

Chapter 4 - Implementing a GPT Model From Scratch to Generate Text

- Time to assemble the building blocks of an LLM into a GPT-like model used to generate text
- In this chapter, we implement the other parts of an LLM architecture beyond data preparation, sampling, and attention mechanisms

Coding an LLM architecture
- LLMs are large deep neural network architectures designed to generate one new text token at a time
- Not that complex; many of its components are merely scaled/repeated
- Now is the time to implement the core structure, including its transformer blocks
- Will be scaling up embedding dimensions to the size of a small GPT-2 model, with 124 million parameters (trainable weights)
- Later on, we will focus on loading retrained weights into our implementation, adapting it for larger models

- Parameters = trainable weights = internal variables of the model adjusted/optimised to minimise a specific loss function
- In a 2,048^2 tensor neural network, each element is a parameter. In this instances the layer would have 4,194,304 params.

- We will start off with the GPT Backbone, a placeholder model
- Next, we implement further building blocks: Layer normalisation; GELU activation; Feed forward network; Shortcut connections
- These will be combined into a single Transformer block alongside the MultiHeadAttention module we built earlier
- These will be use used to implement the untrained GPT model

- First we build up a DummyGPTModel
- Consists of token and positional embeddings, dropout, a bunch of useless placeholder blocks, a normalisation layer, and an output layer
- Forward method describes data flow through the model: computes token and positional embeddings, applies dropout, processes data through the transformer blocks, normalises, and produces logits
- Next, we will initialise with new input data; first, by-election tokenising a batch consisting of two text inputs

import tiktoken
tokeniser = tiktoken.get_encoding("gpt2")
batch = []
txt1 = "Every effort moves you"
txt2 = "Every day holds a"

batch.append(torch.tensor(tokeniser.encode(txt1)))
batch.append(torch.tensor(tokeniser.encode(txt2)))
batch = torch.stack(batch, dim=0)
print(batch)

- This produces token IDs. This is how it works:
1. Input text (“Every step moves you”)
2. Input text is tokenised into individual words
3. Tokens are turned into embedding vectors of, say, 768 dimensions
4. Passed through the GPT model
5. Embedding vectors are returned as output vectors
6. These are post processed and turned into output text (“Every step moves you forward.”)

- Next, we initialise a 124-million-parameter DummyGPTModel instance and feed it some tokens

torch.manual_seed(123)
model = DummyGPTModel(GPT_CONFIG_124M)
logits = model(batch)
print("Output shape:", logits.shape)
print(logits)

- The output tensor has two rows, for the two text samples. Each text sample consists of four tokens; each token has a 50,257-dim vector (reflective of the size of the vocabulary)
- In postprocessinng, these vectors will be reconverted back into token IDs, and then decoded into words

Normalising Activiations with Layer Normalisation
- Networks have difficulty learning underlying patterns in the data to a degree that would allow accurate predictions/decisions - this is due to vanishing/exploding gradients, leading to difficulty in weight adjustment
- We implement layer normalisation to improve stability and efficiency. We adjust outputs to have a mean of 0 and variance of 1. This speeds up convergence to effective weights and ensures consistent training.
- Usually applied before and after multi-head attention module, and before the final output layer.

torch.manual_seed(123)
batch_example = torch.randn(2, 5)
layer = nn.Sequential (nn.Linear(5, 6), nn.ReLU())
out = layer(batch_example)
#print(out)

- We use ReLU (rectified linear unit), standard activation function in NN. It thresholds negative inputs to 0, meaning all outputs are positive.
- We use keepdim in subsequent commands to guarantee that the output tensor dims are equivalent to the input tensor dims
- Next, we apply layer normalisation, involving subtracting the mean ad dividing by the square root of the variance (i.e. the standard deviation)

out_norm = (out - mean) / torch.sqrt(var)
mean = out_norm.mean(dim=-1, keepdim=True)
var = out_norm.var(dim=-1, keepdim=True)
print("Normalised layer outputs:\n", out_norm)
print("Mean:\n", mean)
print("Variance:\n", var)

Output:
Normalised layer outputs:
 tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],
        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],
       grad_fn=<DivBackward0>)
Mean:
 tensor([[9.9341e-09],
        [0.0000e+00]], grad_fn=<MeanBackward1>)
Variance:
 tensor([[1.0000],
        [1.0000]], grad_fn=<VarBackward0>)

- Having applied normalisation step-by-step, we can encapsulate it in a PyTorch module we can use in our GPT model to come

class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift

- Variance measures the spread of the features in the last dimension of 𝑥. Variance is critical for normalizing the input so that it has consistent scale and mean, helping stabilize training. Adding 𝜖 avoids numerical issues (like division by zero).Learnable scale and shift parameters ensure that the model retains flexibility after normalization.

Implementing a Feed Forward Network with GELU Activations
- Now, time to implement a small neural network submodule used in a transformer block
- Starts with the GELU activation function
- ReLU is often used in deep learning — simple and effective in the matter to threshold negative inputs to zero
- GELU (Gaussian Error Linear Unit and SwiGLU (Swish-gated linear unit) are two alternatives. More complex, smoother, incorporating Gaussian and sigmoid-gated linear units respectively
- GELU can be implemented in several ways, ad ca be defined as x [dot-product] comulative distribution function of the standard Gaussian distribution.
- Can be cheaper to implement an approximation, via a PyTorch module.

class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x + 3, 3))))


gelu, relu = GELU(), nn.ReLU()

# Generate 100 evenly spaced points between -3 and 3 to represent input values for the activation functions.
x = torch.linspace(-3, 3, 100)

# Apply the GELU and ReLU activation functions to the input values.
# `gelu(x)` computes the Gaussian Error Linear Unit activation for each value in `x`.
# `relu(x)` computes the Rectified Linear Unit activation for each value in `x`.
y_gelu, y_relu = gelu(x), relu(x)

# Create a new figure with a specified size of 8x3 inches to plot the activation functions.
plt.figure(figsize=(8, 3))

# Loop through the activation functions and their labels to create two subplots.
for i, (y, label) in enumerate(zip([y_gelu, y_relu], ["GELU", "ReLU"]), 1):
    # Create a subplot (1 row, 2 columns, i-th plot).
    plt.subplot(1, 2, i)
    # Plot the input `x` values against the corresponding activation outputs `y`.
    plt.plot(x, y)
    # Set the title of the current subplot to indicate the activation function being plotted.
    plt.title(f"{label} activation function")
    # Label the x-axis as "x" (input to the activation function).
    plt.xlabel("x")
    # Label the y-axis to indicate the output of the corresponding activation function.
    plt.ylabel(f"{label} (x)")
    # Add a grid to the subplot for better visualization of the plot.
    plt.grid(True)

# Adjust the spacing between subplots to prevent overlapping labels and titles.
plt.tight_layout()

# Display the figure with the plotted activation functions.
plt.show()

￼
- This is the output. ReLU is a piecewise linear function that outputs the input directly if it’s positive; otherwise, it outputs zero
- GELU is a smooth, non-linear function that approximates ReLU but with a non-zero gradient for negative values.
- This allows for more nuanced adjustments to the model’s parameters and can mobilise negative input. This makes GELU great for networks with high depth or complex architectures.
- Next, we’ll use GELU to implement the small neural network module, FeedForward, that will be used in the transformer block later

class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)

- This is a small NN consisting of two Linear layers and a GELU activation function.
- In a 124m param GPT model, it receives input batches with tokens with an embedding size of 768 via the GPT_CONFIG_124M dictionary.
- Say we have an input tensor with shape (2,3,768). The three values represent batch size, number of tokens, and embedding dimensions.
- First linear layer increases embedding dimension by a factor of 4, to 3072
- Second linear layer decreases the embedding dimension by a factor of 4, back to 768.
- FeedForward is crucial in enhancing the model’s ability to learn from and generalise data. It inputs and outputs the same dimensions, but between them it expands them into a higher dimensional space through the first linear layer, followed by nonlinear GELU activation and a contraction back to the original dimension. Allows for a richer representation space.
- Also simplifies the architecture by enabling stacking of multiple layers with no dim adjustment.

- We have now implemented most of the LLM’s building blocks! Now, it’s time to investigate shortcut connections inserted between different layers in a neural network, for improving training.

Adding shortcut connections
- Shortcut connections, or skip/residual connections
- Proposed to mitigate vanishing gradients (where vanishing gradient denotes the phenomenon where the loss function, propagated through the network, is almost flat, meaning weights in earlier layers do not get updated enough or at all
- Shortcut connection creates an alternative gradient path by skipping one or more layers, by adding output of one layer to the output of a later layer


class ExampleDeepNeuralNetwork(nn.Module):
    #Deep NN with five layers, each consisting of a Linear layer and a GELU activation function
    #Forward pass, iteratively pass through layers and add shortcuts where shortcut is set to True
    def __init__(self, layer_sizes, use_shortcut):
        super().__init__()
        self.use_shortcut = use_shortcut
        self.layers = nn.ModuleList([
            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2], GELU())),
            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())
        ])

    def forward(self, x):
        for layer in self.layers:
            layer_output = layer(x) #Compute the output of the current layer
            if self.use_shortcut and x.shape == layer_output.shape: #See if shortcut can be applied
                x = x + layer_output
            else:
                x = layer_output
        return x

layer_sizes = [3, 3, 3, 3, 3, 1]
sample_input = torch.tensor ([1., 0., -1.])
torch.manual_seed(123) #Specifics random seed for initial weights for reproducibility
model_without_shortcut = ExampleDeepNeuralNetwork(
    layer_sizes, use_shortcut = False
)

def print_gradients(model, x):
    #Code specifies a loss function that computes how close the model output and a user-specified target are
    output = model(x) #Forward pass
    target = torch.tensor([0.])

    loss = nn.MSELoss()
    loss = loss(output, target) #Calculates loss based on how close the target and output are

    loss.backward()

    for name, param in model.named_parameters():
        if 'weight' in name:
            print(f"{name} has gradient mean of {param.grad.abs().mean().item()}")


print_gradients(model_without_shortcut, sample_input)

This produces the output:

layers.0.0.weight has gradient mean of 0.0020389005076140165
layers.1.0.weight has gradient mean of 0.0013754875399172306
layers.2.0.weight has gradient mean of 0.004195831250399351
layers.3.0.weight has gradient mean of 0.005827412009239197
layers.4.0.weight has gradient mean of 0.01759558729827404

- From this output, it is clear that from weight 4.0 to weight 0, the gradients become smaller and smaller until they almost vanish.
- A model with skip connections compares more favourably:

torch.manual_seed(123)
model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)
print_gradients(model_with_shortcut, sample_input)


layers.0.0.weight has gradient mean of 0.19594410061836243
layers.1.0.weight has gradient mean of 0.6174639463424683
layers.2.0.weight has gradient mean of 0.3319396674633026
layers.3.0.weight has gradient mean of 0.3728732168674469
layers.4.0.weight has gradient mean of 1.1185013055801392

- The last layer has a larger gradient than the others, but the value stabilises as we progress towards layer 1. Gradient flow has been ensured, which is key for training our model

Connecting attention and linear layers in a transformer block
- Time to implement the transformer block, a fundamental building block of GPT/LLM architectures
- This block, repeated 12 times in the 124-param GPT-2 architecture, combines multihead attention, layer normalisation, dropout, feed forward, GELU activations
- When a block processes an input sequence, each element in the sequence is represented by a fixed-size vector of 768 dimensions
- Operations within the block, like multi-head attention and feed forward layers, to transform the vectors while preserving dimensionality.

class TransformerBlock(nn.module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = self.drop_shortcut(x)
        x = x + shortcut

        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut
        return x

- Defines a block class in PyTorch inclusive of a multi-head attention mechanism and feed forward, configured based on a provided configuration dictionary (cfg)
- Layer normalisation is applied before each of these two components, dropout is applied after them to regularise the model and prevent overfitting.
- Implements the forward pass, with each component followed by a shortcut connection adding the input of a block to its output
- Instantiating an instance for testing works like so:

torch.manual_seed(123)
x = torch.rand(2, 4, 768)
block = TransformerBlock(GPT_CONFIG_124M)
output = block(x)

print("Input shape:", x.shape)
print("Output shape:", output.shape)

Output:
Input shape: torch.Size([2, 4, 768])
Output shape: torch.Size([2, 4, 768])

- The block maintains the input dims in output
- Preservation of shape thus is a critical aspect of transformer design. Enables effective application across a range of sequence-to-sequence tasks, with all output vectors corresponding to input vectors
- However, the output is a context vector that encapsulates information from the entire input sequence. Thus, while physical dimensions of sequence remain the same, the content is modified to integrate contextual information from across the sequence.
- We now have all the building blocks required to implement the GPT architecture.

——————————————————————————————————————————————————————————


——————————————————————————————————————————————————————————

Questions:

1. When passing through the fully connected layer, how do the 5 inputs become 6 features? Is it via dot product of each input by the weights, as it would be in a regular self-attention mechanism?

ChatGPT
Excellent question, Maxi! The transformation from 5 inputs to 6 features in a fully connected layer (nn.Linear) is not quite the same as a dot product in self-attention, but the principle is related. Here's a detailed breakdown:

How the Transformation Works in nn.Linear
When you pass a tensor through a fully connected layer (nn.Linear), the 5 inputs are transformed into 6 outputs via matrix multiplication with the weight matrix and addition of a bias vector. This operation computes a weighted sum of the input features for each of the 6 output features.

￼

Adding the Bias: After the weighted sum, a bias term 𝑏 is added element-wise to each output feature. This gives the final linear transformation output:

How This Differs From Self-Attention
In self-attention mechanisms, the dot product serves a different role. Instead of transforming the inputs into a new feature space, the dot product is used to compute the similarity between queries. In contrast, nn.Linear is a pure transformation mechanism that maps input features to a new feature space, without computing token-to-token similarities.

Why Do the Inputs Expand to 6 Outputs?

The increase from 5 to 6 features is controlled by the number of output features specified in nn.Linear(5, 6).

This operation allows the model to:

- Increase Dimensionality:Project the input into a higher-dimensional space to capture more complex patterns.

- Learn Feature Transformations: The weights and biases are trainable, meaning the model learns how to combine and transform the 5 input features to produce 6 meaningful outputs during training.




